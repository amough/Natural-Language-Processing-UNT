{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-K9BsX3KcC6y"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8hFNQohPORn"
      },
      "source": [
        "# **ICE-3: Text Preprocessing Beyond Tokenization**\n",
        "**Instructions**:\n",
        "1. Wherever you are asked to code, insert a text block below your code block and explain what you have coded as per your own understanding.\n",
        "2. If the code is provided by us, execute it, and add below a text block and provide your explanation in your own words.\n",
        "3. Submit both the .ipynb and pdf files to canvas.\n",
        "4. **The similarity score should be less than 15%.**\n",
        "\n",
        "This notebook focuses on preprocessing English text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peiyYN47rMy1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f9fa0cc-e7b1-4bd1-f8d8-0a2be9d0c992"
      },
      "source": [
        "import re\n",
        "\n",
        "# for using NLTK\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')#I have added this because I was getting an error in WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# for using SpaCy \n",
        "import spacy\n",
        "\n",
        "# for HuggingFace\n",
        "!pip install transformers\n",
        "# !pip install ftfy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.22.1-py3-none-any.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 5.1 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 48.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Collecting huggingface-hub<1.0,>=0.9.0\n",
            "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 56.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.9.1 tokenizers-0.12.1 transformers-4.22.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "all the moduels and libraries that are required are being downloaded/imported.\n",
        "re, nltk and spacy are imported.\n",
        "I have added nltk.download('omw-1.4') because I was getting an error in WordNetLemmatizer"
      ],
      "metadata": {
        "id": "lVPPy2uPC3vK"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3Bh5T8sJXFQ"
      },
      "source": [
        "# trick to wrap text to the viewing window for this notebook\n",
        "# Ref: https://stackoverflow.com/questions/58890109/line-wrapping-in-collaboratory-google-results\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "a user-defined function set_css has been defined."
      ],
      "metadata": {
        "id": "BNaQjsVVDUSg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxtJzrzGXchm"
      },
      "source": [
        "## **(Tutorial) Tokenizing text using Spacy**\n",
        "\n",
        "Following is a sample of text to demonstrate tokenization in SpaCy. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KP9CaD6ufac0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "outputId": "f57822e0-a42b-4321-8bb0-a76c446be3f2"
      },
      "source": [
        "dummy_text1 = \"\"\"Here is the First Paragraph and this is the First Sentence. Here is the Second Sentence. Now is the Third Sentence. This is the Fourth Sentence of the first paragaraph. This paragraph is ending now with a Fifth Sentence.\n",
        "Now, it is the Second Paragraph and its First Sentence. Here is the Second Sentence. Now is the Third Sentence. This is the Fourth Sentence of the second paragraph. This paragraph is ending now with a Fifth Sentence.\n",
        "Finally, this is the Third Paragraph and is the First Sentence of this paragraph. Here is the Second Sentence. Now is the Third Sentence. This is the Fourth Sentence of the third paragaraph. This paragraph is ending now with a Fifth Sentence.\n",
        "4th paragraph just has one sentence in it.\n",
        "\"\"\"\n",
        "\n",
        "print(dummy_text1)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is the First Paragraph and this is the First Sentence. Here is the Second Sentence. Now is the Third Sentence. This is the Fourth Sentence of the first paragaraph. This paragraph is ending now with a Fifth Sentence.\n",
            "Now, it is the Second Paragraph and its First Sentence. Here is the Second Sentence. Now is the Third Sentence. This is the Fourth Sentence of the second paragraph. This paragraph is ending now with a Fifth Sentence.\n",
            "Finally, this is the Third Paragraph and is the First Sentence of this paragraph. Here is the Second Sentence. Now is the Third Sentence. This is the Fourth Sentence of the third paragaraph. This paragraph is ending now with a Fifth Sentence.\n",
            "4th paragraph just has one sentence in it.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dummy_text1 is being displayed."
      ],
      "metadata": {
        "id": "yp9IpgrBD3YL"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6SZCwFAfxfI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "be685aed-c20a-4038-fedb-c216b496be2c"
      },
      "source": [
        "# loads a trained English pipeline with specific preprocessing components\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# using SpaCy's tokenizer...\n",
        "doc = nlp(dummy_text1)      # applies the processing pipeline on the text\n",
        "for token in doc:\n",
        "  print(token.text)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here\n",
            "is\n",
            "the\n",
            "First\n",
            "Paragraph\n",
            "and\n",
            "this\n",
            "is\n",
            "the\n",
            "First\n",
            "Sentence\n",
            ".\n",
            "Here\n",
            "is\n",
            "the\n",
            "Second\n",
            "Sentence\n",
            ".\n",
            "Now\n",
            "is\n",
            "the\n",
            "Third\n",
            "Sentence\n",
            ".\n",
            "This\n",
            "is\n",
            "the\n",
            "Fourth\n",
            "Sentence\n",
            "of\n",
            "the\n",
            "first\n",
            "paragaraph\n",
            ".\n",
            "This\n",
            "paragraph\n",
            "is\n",
            "ending\n",
            "now\n",
            "with\n",
            "a\n",
            "Fifth\n",
            "Sentence\n",
            ".\n",
            "\n",
            "\n",
            "Now\n",
            ",\n",
            "it\n",
            "is\n",
            "the\n",
            "Second\n",
            "Paragraph\n",
            "and\n",
            "its\n",
            "First\n",
            "Sentence\n",
            ".\n",
            "Here\n",
            "is\n",
            "the\n",
            "Second\n",
            "Sentence\n",
            ".\n",
            "Now\n",
            "is\n",
            "the\n",
            "Third\n",
            "Sentence\n",
            ".\n",
            "This\n",
            "is\n",
            "the\n",
            "Fourth\n",
            "Sentence\n",
            "of\n",
            "the\n",
            "second\n",
            "paragraph\n",
            ".\n",
            "This\n",
            "paragraph\n",
            "is\n",
            "ending\n",
            "now\n",
            "with\n",
            "a\n",
            "Fifth\n",
            "Sentence\n",
            ".\n",
            "\n",
            "\n",
            "Finally\n",
            ",\n",
            "this\n",
            "is\n",
            "the\n",
            "Third\n",
            "Paragraph\n",
            "and\n",
            "is\n",
            "the\n",
            "First\n",
            "Sentence\n",
            "of\n",
            "this\n",
            "paragraph\n",
            ".\n",
            "Here\n",
            "is\n",
            "the\n",
            "Second\n",
            "Sentence\n",
            ".\n",
            "Now\n",
            "is\n",
            "the\n",
            "Third\n",
            "Sentence\n",
            ".\n",
            "This\n",
            "is\n",
            "the\n",
            "Fourth\n",
            "Sentence\n",
            "of\n",
            "the\n",
            "third\n",
            "paragaraph\n",
            ".\n",
            "This\n",
            "paragraph\n",
            "is\n",
            "ending\n",
            "now\n",
            "with\n",
            "a\n",
            "Fifth\n",
            "Sentence\n",
            ".\n",
            "\n",
            "\n",
            "4th\n",
            "paragraph\n",
            "just\n",
            "has\n",
            "one\n",
            "sentence\n",
            "in\n",
            "it\n",
            ".\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "using spacy's tokenizer, dummy_text1 is being tokenized and then those tokens are displayed"
      ],
      "metadata": {
        "id": "J7qApK4gEMNH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWJsFBrePj_o"
      },
      "source": [
        "### **Task 1. Revisiting Tokenization**\n",
        "\n",
        "Whitespace-based tokenization is a naive approach to tokenize text, where the idea is to extract words that are separated by whitespace characters on either sides.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci59Ry_hQq_C"
      },
      "source": [
        "#### **Question 1a. Implement the naive approach of tokenizing words (whitespace-based) for the text given in the code block below.(5 points)** \n",
        "\n",
        "**Important Note:** \n",
        "1. DO NOT use any of the existing implementations for tokenization distributed as part of open-source NLP libraries.\n",
        "2. **If your solution uses readily available implementations of tokenizers, you will receive zero credit for this question.**\n",
        "3. Your tokenizer implentation need not be the most optimized one. It should just be able to get the job done. You can also ignore punctuation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQI_o_IEfR5j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "2f8ff17d-5037-4758-b12f-66cbd63da7b4"
      },
      "source": [
        "sample_text=\"\"\"Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, \n",
        "when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting,\n",
        "remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software\n",
        "like Aldus PageMaker including versions of Lorem Ipsum.\"\"\"\n",
        "\n",
        "# add your code below this comment and execute it once you have written the code\n",
        "re_pattern2 = r'[A-Z][a-z]+|[a-z]+|[0-9]+[a-z]*'\n",
        "print(re.findall(re_pattern2, sample_text))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Lorem', 'Ipsum', 'is', 'simply', 'dummy', 'text', 'of', 'the', 'printing', 'and', 'typesetting', 'industry', 'Lorem', 'Ipsum', 'has', 'been', 'the', 'industry', 's', 'standard', 'dummy', 'text', 'ever', 'since', 'the', '1500s', 'when', 'an', 'unknown', 'printer', 'took', 'a', 'galley', 'of', 'type', 'and', 'scrambled', 'it', 'to', 'make', 'a', 'type', 'specimen', 'book', 'It', 'has', 'survived', 'not', 'only', 'five', 'centuries', 'but', 'also', 'the', 'leap', 'into', 'electronic', 'typesetting', 'remaining', 'essentially', 'unchanged', 'It', 'was', 'popularised', 'in', 'the', '1960s', 'with', 'the', 'release', 'of', 'Letraset', 'sheets', 'containing', 'Lorem', 'Ipsum', 'passages', 'and', 'more', 'recently', 'with', 'desktop', 'publishing', 'software', 'like', 'Aldus', 'Page', 'Maker', 'including', 'versions', 'of', 'Lorem', 'Ipsum']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used regualar expression for tokenization. It checks for all the words starting with lowercase and uppercase letters.It also checks for nunbers with 0 or more alphabets at the end."
      ],
      "metadata": {
        "id": "4BW63bsJF6n_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jC3EqwmTSswZ"
      },
      "source": [
        "#### **Question 1b. For the same text in Q1., apply the tokenizers listed below. Analyze how the words are being tokenized by each of the tokenizers. Compare and contrast the outputs of the tokenization schemes.(10 points)**\n",
        "1. **SpaCy's tokenizer**\n",
        "2. **NLTK's tokenizer**\n",
        "\n",
        "**Note:** You are already familiar with using NLTK's tokenization which was demosntrated in the previous labs. If you do not remember, just revisit them to refresh your memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8nGNxYKQqeJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5374e473-a033-4784-b367-7042bd31c25a"
      },
      "source": [
        "sample_text=\"\"\"Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, \n",
        "when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting,\n",
        "remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software\n",
        "like Aldus PageMaker including versions of Lorem Ipsum.\"\"\"\n",
        "\n",
        "# add your code below this comment and execute it once you have written the code.\n",
        "# you can additional code cells if need be. make sure to use the text cell provided to answer the question.\n",
        "#1.Spacy\n",
        "spa = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "## tokenization\n",
        "doc = spa(sample_text)\n",
        "for token in doc:\n",
        "    print(token.text)\n",
        "\n",
        "\n",
        "#2.NLTK\n",
        "tokens = [t for t in sample_text.split()]\n",
        "print(tokens)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lorem\n",
            "Ipsum\n",
            "is\n",
            "simply\n",
            "dummy\n",
            "text\n",
            "of\n",
            "the\n",
            "printing\n",
            "and\n",
            "typesetting\n",
            "industry\n",
            ".\n",
            "Lorem\n",
            "Ipsum\n",
            "has\n",
            "been\n",
            "the\n",
            "industry\n",
            "'s\n",
            "standard\n",
            "dummy\n",
            "text\n",
            "ever\n",
            "since\n",
            "the\n",
            "1500s\n",
            ",\n",
            "\n",
            "\n",
            "when\n",
            "an\n",
            "unknown\n",
            "printer\n",
            "took\n",
            "a\n",
            "galley\n",
            "of\n",
            "type\n",
            "and\n",
            "scrambled\n",
            "it\n",
            "to\n",
            "make\n",
            "a\n",
            "type\n",
            "specimen\n",
            "book\n",
            ".\n",
            "It\n",
            "has\n",
            "survived\n",
            "not\n",
            "only\n",
            "five\n",
            "centuries\n",
            ",\n",
            "but\n",
            "also\n",
            "the\n",
            "leap\n",
            "into\n",
            "electronic\n",
            "typesetting\n",
            ",\n",
            "\n",
            "\n",
            "remaining\n",
            "essentially\n",
            "unchanged\n",
            ".\n",
            "It\n",
            "was\n",
            "popularised\n",
            "in\n",
            "the\n",
            "1960s\n",
            "with\n",
            "the\n",
            "release\n",
            "of\n",
            "Letraset\n",
            "sheets\n",
            "containing\n",
            "Lorem\n",
            "Ipsum\n",
            "passages\n",
            ",\n",
            "and\n",
            "more\n",
            "recently\n",
            "with\n",
            "desktop\n",
            "publishing\n",
            "software\n",
            "\n",
            "\n",
            "like\n",
            "Aldus\n",
            "PageMaker\n",
            "including\n",
            "versions\n",
            "of\n",
            "Lorem\n",
            "Ipsum\n",
            ".\n",
            "['Lorem', 'Ipsum', 'is', 'simply', 'dummy', 'text', 'of', 'the', 'printing', 'and', 'typesetting', 'industry.', 'Lorem', 'Ipsum', 'has', 'been', 'the', \"industry's\", 'standard', 'dummy', 'text', 'ever', 'since', 'the', '1500s,', 'when', 'an', 'unknown', 'printer', 'took', 'a', 'galley', 'of', 'type', 'and', 'scrambled', 'it', 'to', 'make', 'a', 'type', 'specimen', 'book.', 'It', 'has', 'survived', 'not', 'only', 'five', 'centuries,', 'but', 'also', 'the', 'leap', 'into', 'electronic', 'typesetting,', 'remaining', 'essentially', 'unchanged.', 'It', 'was', 'popularised', 'in', 'the', '1960s', 'with', 'the', 'release', 'of', 'Letraset', 'sheets', 'containing', 'Lorem', 'Ipsum', 'passages,', 'and', 'more', 'recently', 'with', 'desktop', 'publishing', 'software', 'like', 'Aldus', 'PageMaker', 'including', 'versions', 'of', 'Lorem', 'Ipsum.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SLPwJPOXKf9"
      },
      "source": [
        "**Answer for Q1b.** Type in your answer here!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The spcay tokenizer is considering every puncuation and even multiple white-spaces as tokens.\n",
        "The NLTK Tokenizer is only considering a token if it is separated by a whtie-space, otherwise it is asssuming that the punctuation is a part of the word like in the last word \"Ipsum.\" the character '.' is considered a part of Ipsum because they weren't separated by a white-space. It also not considering multiple white-spaces as tokens, which is a good thing."
      ],
      "metadata": {
        "id": "8VvQYmDk-w8l"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgaX-Ck7YYzY"
      },
      "source": [
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNUmlAHIQ9gs"
      },
      "source": [
        "## **(Tutorial) Stemming and Lemmatization using NLTK**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jCebsYwiQxf"
      },
      "source": [
        "Let's see how we can perform stemming and lemmatization using NLTK library..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3M0mDUIiSdu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f13ba0ce-1e14-4d38-a778-796bc1267d11"
      },
      "source": [
        "# importing PorterStemmer class from nltk.stem module\n",
        "from nltk.stem import PorterStemmer\n",
        "porter = PorterStemmer()    # instantiating an object of the PorterStemmer class\n",
        "\n",
        "stem = porter.stem('cats')    # calling the stemmer algorithm on the desired word\n",
        "print(f\"'cats' after stemming: {stem}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'cats' after stemming: cat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "cats was stemmed using PorterStemmer."
      ],
      "metadata": {
        "id": "Ilpn02zBzCjB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Try executing the portstemmer stemmer on your own examples (2 points)**"
      ],
      "metadata": {
        "id": "H_p-k7WyAj59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Enter your code here\n",
        "stem = porter.stem('driving')    # calling the stemmer algorithm on the desired word\n",
        "print(f\"'driving' after stemming: {stem}\")\n",
        "\n",
        "stem = porter.stem('flying')    # calling the stemmer algorithm on the desired word\n",
        "print(f\"'flying' after stemming: {stem}\")\n",
        "\n",
        "stem = porter.stem('animals')    # calling the stemmer algorithm on the desired word\n",
        "print(f\"'animals' after stemming: {stem}\")\n",
        "\n",
        "stem = porter.stem('dogs')    # calling the stemmer algorithm on the desired word\n",
        "print(f\"'dogs' after stemming: {stem}\")"
      ],
      "metadata": {
        "id": "Mjmkh8h5--gU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "4a66ac1d-34cc-46c4-f36b-4975f0bfb856"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'driving' after stemming: drive\n",
            "'flying' after stemming: fli\n",
            "'animals' after stemming: anim\n",
            "'dogs' after stemming: dog\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "driving, flying, animals and dogs are being stemmed using Porter Stemmer. dogs and driving gives correct answer, rest all failed."
      ],
      "metadata": {
        "id": "M1wSmT98znCf"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Jo-qgQfjuli",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "22d933b0-3705-4642-851d-0d8557e658b0"
      },
      "source": [
        "# importing WordNet-based lemmatizer class from nltk.stem module\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()    # instantiating an object of the WordNetLemmatizer class\n",
        "\n",
        "lemma = lemmatizer.lemmatize('cats')    # calling the lemmatization algorithm on the desired word\n",
        "print(f\"'cats' after lemmatization: {lemma}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'cats' after lemmatization: cat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Try executing the wordnet based lemmatizer on your own examples (3 points)**"
      ],
      "metadata": {
        "id": "pZg6RcS0A4y4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Enter your code here\n",
        "lemma = lemmatizer.lemmatize('driving')    # calling the lemmatization algorithm on the desired word\n",
        "print(f\"'driving' after lemmatization: {lemma}\")\n",
        "\n",
        "lemma = lemmatizer.lemmatize('flying')    # calling the lemmatization algorithm on the desired word\n",
        "print(f\"'flying' after lemmatization: {lemma}\")\n",
        "\n",
        "lemma = lemmatizer.lemmatize('animals')    # calling the lemmatization algorithm on the desired word\n",
        "print(f\"'animals' after lemmatization: {lemma}\")\n",
        "\n",
        "lemma = lemmatizer.lemmatize('dogs')    # calling the lemmatization algorithm on the desired word\n",
        "print(f\"'dogs' after lemmatization: {lemma}\")"
      ],
      "metadata": {
        "id": "0m8icXUx_SUc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "0670625d-94aa-4b4d-e018-22ceab83bd72"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'driving' after lemmatization: driving\n",
            "'flying' after lemmatization: flying\n",
            "'animals' after lemmatization: animal\n",
            "'dogs' after lemmatization: dog\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "driving, flying, animals and dogs are being lemmatized using wordnet based lemmatizer. dogs and animals gives correct answer, rest all failed."
      ],
      "metadata": {
        "id": "9bRZBnD02Z4C"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDGVeoHqrQkF"
      },
      "source": [
        "### **Task 2: Lemmatization or Stemming?**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3K_-XgVmKin"
      },
      "source": [
        "Following is the text that you will be using for this task (Task 2 only):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCtK0QouYj2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "fa6fa2d1-d838-4717-eb83-1bcde5bfec06"
      },
      "source": [
        "# This is the text on which you have to perform stemming; taken from Internet.\n",
        "text = \"In linguistic morphology and information retrieval, stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form; generally a written word form. The stem need not be identical to the morphological root of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root.\"\n",
        "print(\"Given text:\")\n",
        "print(text)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Given text:\n",
            "In linguistic morphology and information retrieval, stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form; generally a written word form. The stem need not be identical to the morphological root of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "text is displayed"
      ],
      "metadata": {
        "id": "bgpCs09fBQ0U"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxubbVwPmSsO"
      },
      "source": [
        "Performing some preprocessing that we have learnt in previous ICEs..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "el7w7c7HmY9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "297e40fd-1190-4092-bd1f-3935c54931ee"
      },
      "source": [
        "en_stopwords = set(stopwords.words('english'))\n",
        "def remove_punc(text_string):\n",
        "  return re.sub('[^a-zA-Z0-9 ]', '', text_string.lower())\n",
        "\n",
        "def remove_stopwords(text_string):\n",
        "  return [ token for token in text_string.split(' ') if token not in en_stopwords ]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions are defined to remove puncuation and stopwords"
      ],
      "metadata": {
        "id": "b6tJvDW7BgoV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ac5uju7eGVfg"
      },
      "source": [
        "#### **Question 2. Remove punctuation and stopwords from the text using the functions provided above.Then perform stemming on the cleaned text using the Porter Stemmer from NLTK.(10 points)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAC8FFLCErdI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "outputId": "065251f5-ae44-4e3a-a7b2-964373d75151"
      },
      "source": [
        "# apply Porter Stemmer on the cleaned text (after punctuation and stopwords are removed) below this comment\n",
        "text = \"In linguistic morphology and information retrieval, stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form; generally a written word form. The stem need not be identical to the morphological root of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root.\"\n",
        "\n",
        "def remove_punc(text_string):\n",
        "  return re.sub('[^a-zA-Z0-9 ]', '', text_string.lower())\n",
        "\n",
        "def remove_stopwords(text_string):\n",
        "  return [ token for token in text_string.split(' ') if token not in en_stopwords ]\n",
        "text = remove_punc(text)\n",
        "clean_text = remove_stopwords(text)\n",
        "#print(clean_text) #I had displayed the clean_text but realized later that it is not required\n",
        "from nltk.stem import PorterStemmer\n",
        "porter = PorterStemmer()    \n",
        "for term in clean_text:\n",
        "  stem = porter.stem(term)\n",
        "  print(stem) "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "linguist\n",
            "morpholog\n",
            "inform\n",
            "retriev\n",
            "stem\n",
            "process\n",
            "reduc\n",
            "inflect\n",
            "sometim\n",
            "deriv\n",
            "word\n",
            "word\n",
            "stem\n",
            "base\n",
            "root\n",
            "form\n",
            "gener\n",
            "written\n",
            "word\n",
            "form\n",
            "stem\n",
            "need\n",
            "ident\n",
            "morpholog\n",
            "root\n",
            "word\n",
            "usual\n",
            "suffici\n",
            "relat\n",
            "word\n",
            "map\n",
            "stem\n",
            "even\n",
            "stem\n",
            "valid\n",
            "root\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "text is cleaned by removing punctuations and stop words. The cleaned text is stemmed using Porter Stemmer from NLTK."
      ],
      "metadata": {
        "id": "OYjKfrJWCz6G"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N44TNDasS6eK"
      },
      "source": [
        "#### **Question 3. Perform lemmatization on the same cleaned text above using NLTK's lemmatizer.(10 points)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJ-moA25Erh3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "outputId": "67114262-4c50-496a-bc05-05982edc9af0"
      },
      "source": [
        "# apply NLTK's lemmatizer on the cleaned text (after punctuation and stopwords are removed) below this comment\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "for word in clean_text:\n",
        "  lemma = lemmatizer.lemmatize(word)\n",
        "  print(lemma) "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "linguistic\n",
            "morphology\n",
            "information\n",
            "retrieval\n",
            "stemming\n",
            "process\n",
            "reducing\n",
            "inflected\n",
            "sometimes\n",
            "derived\n",
            "word\n",
            "word\n",
            "stem\n",
            "base\n",
            "root\n",
            "form\n",
            "generally\n",
            "written\n",
            "word\n",
            "form\n",
            "stem\n",
            "need\n",
            "identical\n",
            "morphological\n",
            "root\n",
            "word\n",
            "usually\n",
            "sufficient\n",
            "related\n",
            "word\n",
            "map\n",
            "stem\n",
            "even\n",
            "stem\n",
            "valid\n",
            "root\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same clean_text is used as Question 2. The cleaned data is lemmatized using wordnet based lemmatizer from NLTK"
      ],
      "metadata": {
        "id": "wn2CHay2D3JE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v93jILmOT9mG"
      },
      "source": [
        "#### **Question 4. How good is Lemmatization when compared to Stemming? Also write down your observations on performing lemmatization and stemming on text before and after cleaning (removing punctuation and stopwords) (10 points)**\n",
        "\n",
        "**IMPORTANT NOTE: Your observations should not be based on just Q2 and Q3. Your observations should characterize spacy's and nltk's segmentation as a whole. Bring out the differences also if there are any**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS-ZjxoOURAJ"
      },
      "source": [
        "**Answer for Q4.:** Type your answer here!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJNe9XfmOcqi"
      },
      "source": [
        "Lemmatization is pretty good as compared to Stemming in some cases but vice versa is also true. Both Lemmatization and Stemming perform better than in each other for some cases. A good example is that PorterStemmer worked for 'driving' but WordNet based Lemmatizer failed, and similarly WordNet based Lemmatizer worked for 'animals' but Porter Stemmer failed. They both worked for 'dogs' but both failed for 'flying'. So we can clearly see that there is no clear winner between the two, it majorly depends upon the data and application. I think stemming is performing similar before and after cleaning the data but Lemmatizer is performing better after cleaning the data. Stemming removes affixes while Lemmatizer converts the words to base form. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lixVq2oOP1VP"
      },
      "source": [
        "## **(Tutorial) Sentence Segmentation using Spacy**\n",
        "\n",
        "Following is a dummy paragraph of text to demonstrate how to use SpaCy to segment text into sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuFV_nccQq6u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "outputId": "4a3bcaa6-ce37-43fb-8243-301a135c0eb7"
      },
      "source": [
        "dummy_text3 = \"\"\"Here is the First Paragraph and this is the First Sentence. Here is the Second Sentence. Now is the Third Sentence. This is the Fourth Sentence of the first paragaraph. This paragraph is ending now with a Fifth Sentence.\n",
        "Now, it is the Second Paragraph and its First Sentence. Here is the Second Sentence. Now is the Third Sentence. This is the Fourth Sentence of the second paragraph. This paragraph is ending now with a Fifth Sentence.\n",
        "Finally, this is the Third Paragraph and is the First Sentence of this paragraph. Here is the Second Sentence. Now is the Third Sentence. This is the Fourth Sentence of the third paragaraph. This paragraph is ending now with a Fifth Sentence.\n",
        "4th paragraph just has one sentence in it.\n",
        "\"\"\"\n",
        "\n",
        "print(dummy_text3)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is the First Paragraph and this is the First Sentence. Here is the Second Sentence. Now is the Third Sentence. This is the Fourth Sentence of the first paragaraph. This paragraph is ending now with a Fifth Sentence.\n",
            "Now, it is the Second Paragraph and its First Sentence. Here is the Second Sentence. Now is the Third Sentence. This is the Fourth Sentence of the second paragraph. This paragraph is ending now with a Fifth Sentence.\n",
            "Finally, this is the Third Paragraph and is the First Sentence of this paragraph. Here is the Second Sentence. Now is the Third Sentence. This is the Fourth Sentence of the third paragaraph. This paragraph is ending now with a Fifth Sentence.\n",
            "4th paragraph just has one sentence in it.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dummy_text3 is displayed"
      ],
      "metadata": {
        "id": "2mJfYG0uKBXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code for sentence segmentation using Spacy**"
      ],
      "metadata": {
        "id": "1BusKIBxKydQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtuBXdrAQC94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "89079acb-1416-4d7a-c378-d0d17df5bcd3"
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# performing sentence splitting...\n",
        "doc = nlp(dummy_text3)\n",
        "for sentence in doc.sents:\n",
        "  print(sentence)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is the First Paragraph and this is the First Sentence.\n",
            "Here is the Second Sentence.\n",
            "Now is the Third Sentence.\n",
            "This is the Fourth Sentence of the first paragaraph.\n",
            "This paragraph is ending now with a Fifth Sentence.\n",
            "\n",
            "Now, it is the Second Paragraph and its First Sentence.\n",
            "Here is the Second Sentence.\n",
            "Now is the Third Sentence.\n",
            "This is the Fourth Sentence of the second paragraph.\n",
            "This paragraph is ending now with a Fifth Sentence.\n",
            "\n",
            "Finally, this is the Third Paragraph and is the First Sentence of this paragraph.\n",
            "Here is the Second Sentence.\n",
            "Now is the Third Sentence.\n",
            "This is the Fourth Sentence of the third paragaraph.\n",
            "This paragraph is ending now with a Fifth Sentence.\n",
            "\n",
            "4th paragraph just has one sentence in it.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dummy_text3 is segmented using spacy."
      ],
      "metadata": {
        "id": "PlBs-DgkKKHq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code for sentence segmentation using NLTK library**"
      ],
      "metadata": {
        "id": "3cbtvu9uFyIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "Kq3wLQfUFihv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "9a0e6666-e02d-4e5a-f858-b968d12fb037"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "nltk is imported and package punkt is downloaded/updated"
      ],
      "metadata": {
        "id": "8wSUnOt0Kc6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"This is a very bad situation. Also I am looking good\"\n",
        "sentences=nltk.sent_tokenize(text)\n",
        "for sentence in sentences:\n",
        "  print(sentence)\n",
        "  print()"
      ],
      "metadata": {
        "id": "5VTLSZMdFFQl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "b303e089-6d70-4ad1-baab-bedae7edb14a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a very bad situation.\n",
            "\n",
            "Also I am looking good\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "text is segmented using NLTK"
      ],
      "metadata": {
        "id": "VH1eAPXgKodf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtQeRGq9Q_mz"
      },
      "source": [
        "\n",
        "### **Task 3. Segmenting Sentences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jg4DnVjvSoTi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "4230bb08-51df-4940-ba10-40a186612d89"
      },
      "source": [
        "inau_text=\"\"\"There are many variations of passages of Lorem Ipsum available, but the majority have suffered alteration in some form, by injected humour, or randomised words which\n",
        "don't look even slightly believable. If you are going to use a passage of Lorem Ipsum, you need to be sure there isn't anything embarrassing hidden in the middle of text. All the\n",
        "Lorem Ipsum generators on the Internet tend to repeat predefined chunks as necessary, making this the first true generator on the Internet. It uses a dictionary of over 200 Latin\n",
        "words, combined with a handful of model sentence structures, to generate Lorem Ipsum which looks reasonable. The generated Lorem Ipsum is therefore always free from repetition,\n",
        "injected humour, or non-characteristic words etc.\"\"\"\n",
        "\n",
        "print(inau_text)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are many variations of passages of Lorem Ipsum available, but the majority have suffered alteration in some form, by injected humour, or randomised words which\n",
            "don't look even slightly believable. If you are going to use a passage of Lorem Ipsum, you need to be sure there isn't anything embarrassing hidden in the middle of text. All the\n",
            "Lorem Ipsum generators on the Internet tend to repeat predefined chunks as necessary, making this the first true generator on the Internet. It uses a dictionary of over 200 Latin\n",
            "words, combined with a handful of model sentence structures, to generate Lorem Ipsum which looks reasonable. The generated Lorem Ipsum is therefore always free from repetition,\n",
            "injected humour, or non-characteristic words etc.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "inau_text is displayed"
      ],
      "metadata": {
        "id": "OajPvIyfKt2B"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVHl6V3eVudZ"
      },
      "source": [
        "#### **Question 5a. Implement a custom Python script that performs a simple way of segmenting sentences in the text above by using the period (.) character as the sentence boundary. Analyze the generated output and provide your observations.(15 points)**\n",
        "\n",
        "**Note:** You do not need to remove any stopwords, punctuation or apply any kind of other preprocessing techniques. Only perform what's asked to minimize your effort needed to answer this question. \n",
        "\n",
        "**Hint**: Use print( ) to help you understand how the sentences are being split when analyzing your output to note down your observations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZBRbBDuWogQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "9db5ad0f-a275-4763-d3b2-4c75b816163a"
      },
      "source": [
        "# write your code below this comment\n",
        "\n",
        "for i,w in enumerate(inau_text.split(\". \")):\n",
        "    print(w+\".\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are many variations of passages of Lorem Ipsum available, but the majority have suffered alteration in some form, by injected humour, or randomised words which\n",
            "don't look even slightly believable.\n",
            "If you are going to use a passage of Lorem Ipsum, you need to be sure there isn't anything embarrassing hidden in the middle of text.\n",
            "All the\n",
            "Lorem Ipsum generators on the Internet tend to repeat predefined chunks as necessary, making this the first true generator on the Internet.\n",
            "It uses a dictionary of over 200 Latin\n",
            "words, combined with a handful of model sentence structures, to generate Lorem Ipsum which looks reasonable.\n",
            "The generated Lorem Ipsum is therefore always free from repetition,\n",
            "injected humour, or non-characteristic words etc..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The text is being split in segments by using the period(.) character as the sentence boundary. I have used the split function with '.' as the parameter for splitting. As it was stated so I have not removed any stopwords, punctuations or applied any kind of preprocessing techniques.\n",
        "The output is sentence being displayed after being segmented. I think all the sentences are segmented where '.' was pressent."
      ],
      "metadata": {
        "id": "GYuUG-ftM0Sw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id9ygYT6Wo6b"
      },
      "source": [
        "#### **Question 5b. Using SpaCy, perform sentence segmentation on the same text (that was used in Q5a.). Analyze the generated output and provide your observations.<br> Now implement segmentation using NLTK, provide your observations.(15 points)**\n",
        "\n",
        "**Hint**: For implementing NLTK's sentence segmentation, you can refer to the code block above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Urwzg2zNWpD7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "8bad6f90-203a-4ef2-b306-807fa418bfb1"
      },
      "source": [
        "# write your code below this comment\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# performing sentence splitting...\n",
        "doc = nlp(inau_text)\n",
        "for sentence in doc.sents:\n",
        "  print(sentence)\n",
        "print()\n",
        "print()\n",
        "sentences=nltk.sent_tokenize(inau_text)\n",
        "for sentence in sentences:\n",
        "  print(sentence)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are many variations of passages of Lorem Ipsum available, but the majority have suffered alteration in some form, by injected humour, or randomised words which\n",
            "don't look even slightly believable.\n",
            "If you are going to use a passage of Lorem Ipsum, you need to be sure there isn't anything embarrassing hidden in the middle of text.\n",
            "All the\n",
            "Lorem Ipsum generators on the Internet tend to repeat predefined chunks as necessary, making this the first true generator on the Internet.\n",
            "It uses a dictionary of over 200 Latin\n",
            "words, combined with a handful of model sentence structures, to generate Lorem Ipsum which looks reasonable.\n",
            "The generated Lorem Ipsum is therefore always free from repetition,\n",
            "injected humour, or non-characteristic words etc.\n",
            "\n",
            "\n",
            "There are many variations of passages of Lorem Ipsum available, but the majority have suffered alteration in some form, by injected humour, or randomised words which\n",
            "don't look even slightly believable.\n",
            "If you are going to use a passage of Lorem Ipsum, you need to be sure there isn't anything embarrassing hidden in the middle of text.\n",
            "All the\n",
            "Lorem Ipsum generators on the Internet tend to repeat predefined chunks as necessary, making this the first true generator on the Internet.\n",
            "It uses a dictionary of over 200 Latin\n",
            "words, combined with a handful of model sentence structures, to generate Lorem Ipsum which looks reasonable.\n",
            "The generated Lorem Ipsum is therefore always free from repetition,\n",
            "injected humour, or non-characteristic words etc.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output for Spacy Segmentation is same as the output for my Segmentaiton in Q5a. It might perform better when there is '?' because we never mentioned that we have to '.' as the sentence boundary and it still segmnets the text.\n",
        "\n",
        "I have printed two blank lines in between, so that the output is clear.\n",
        "\n",
        "The output for NLTK Segmentation is same as the output for my Segmentaiton in Q5a and the spacy segmentation. It might perform better than my segmenter when there is '?' because we never mentioned that we have to '.' as the sentence boundary and it still segmnets the text."
      ],
      "metadata": {
        "id": "4gSdMfXEOipI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiEaJ3RuSmeU"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYE8A-Mk4BKe"
      },
      "source": [
        "## **(Tutorial) Subword Tokenization using HuggingFace**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4J8U2pWSc8v"
      },
      "source": [
        "### **Task 4: Subword Tokenization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yS4dlR20V1eX"
      },
      "source": [
        "Well, the natural language processing is not as intelligent as we humans are, and not so intellectual to break words into sub words and try to decipher the word if it sees a word that is not in the corpus yet. This is where Subword Tokenization comes into picture.\n",
        "\n",
        "Subword tokenization is a recent strategy from machine translation that helps us solve these problems by breaking unknown words into “subword units” - strings of characters like ing or eau - that still allow the downstream model to make intelligent decisions on words it doesn't recognize.\n",
        "\n",
        "**Below is the implementation of the Subword Tokenization:**\n",
        "<br>We will see Byte Pair Encoder algorithm here:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers\n",
        "\n",
        "!wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-vocab.json\n",
        "!wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt"
      ],
      "metadata": {
        "id": "EZ6Q1AMKi2Kg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "outputId": "f0bb213a-b6c4-47e4-94b6-042d05baf120"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.7/dist-packages (0.12.1)\n",
            "--2022-09-25 20:51:36--  https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-vocab.json\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.42.38\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.42.38|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1042301 (1018K) [application/json]\n",
            "Saving to: ‘gpt2-medium-vocab.json’\n",
            "\n",
            "gpt2-medium-vocab.j 100%[===================>]   1018K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-09-25 20:51:36 (7.27 MB/s) - ‘gpt2-medium-vocab.json’ saved [1042301/1042301]\n",
            "\n",
            "--2022-09-25 20:51:36--  https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.42.38\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.42.38|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 456318 (446K) [text/plain]\n",
            "Saving to: ‘gpt2-merges.txt’\n",
            "\n",
            "gpt2-merges.txt     100%[===================>] 445.62K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-09-25 20:51:36 (3.98 MB/s) - ‘gpt2-merges.txt’ saved [456318/456318]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizer installed"
      ],
      "metadata": {
        "id": "eS3JZFOgQjBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "gpt2vocab = \"gpt2-medium-vocab.json\"\n",
        "gpt2merges = \"gpt2-merges.txt\"\n",
        "\n",
        "bpe = ByteLevelBPETokenizer(gpt2vocab, gpt2merges)\n",
        "bpe_encoding = bpe.encode(\"The custom of delivering an address on Inauguration Day started with the very first Inauguration—George Washington’s—on April 30, 1789.\")\n",
        "print(bpe_encoding.ids)\n",
        "print(bpe_encoding.tokens)"
      ],
      "metadata": {
        "id": "WbQGR-9XlWUa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "9e7c0ab8-6f68-467f-8ffd-0d0473e99c6b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[464, 2183, 286, 13630, 281, 2209, 319, 554, 7493, 3924, 3596, 2067, 351, 262, 845, 717, 554, 7493, 3924, 960, 20191, 2669, 447, 247, 82, 960, 261, 3035, 1542, 11, 1596, 4531, 13]\n",
            "['The', 'Ġcustom', 'Ġof', 'Ġdelivering', 'Ġan', 'Ġaddress', 'Ġon', 'ĠIn', 'aug', 'uration', 'ĠDay', 'Ġstarted', 'Ġwith', 'Ġthe', 'Ġvery', 'Ġfirst', 'ĠIn', 'aug', 'uration', 'âĢĶ', 'George', 'ĠWashington', 'âĢ', 'Ļ', 's', 'âĢĶ', 'on', 'ĠApril', 'Ġ30', ',', 'Ġ17', '89', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sentence is encoded using the Byte-Pair Encoding Tokenizer."
      ],
      "metadata": {
        "id": "7pJl9yhCQmYt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CyJcXmAkQiBD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6:** Consider the following two sentences:\n",
        "\n",
        "* The movie was not good at all, the climax was good though.\n",
        "* That's an example, don't ignore it!. Or else, you might miss key information.\n",
        "\n",
        "Encode these sentences using the Byte-Pair Encoding tokenizer (created during the tutorial). Retrieve the tokens from the encodings of the two sentences. Is/Are there any interesting observations when you compare the tokens between the two encodings? What do you think is causing what you observe as part of your comparison? **(20points)**\n"
      ],
      "metadata": {
        "id": "jD90aNX4kLAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use the bpe tokenizer that was created during the tutorial to encode the sentences\n",
        "# write your code below this comment and execute\n",
        "# type in your answer to the question asked above in the following cell (see below)\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "gpt2vocab = \"gpt2-medium-vocab.json\"\n",
        "gpt2merges = \"gpt2-merges.txt\"\n",
        "\n",
        "bpe = ByteLevelBPETokenizer(gpt2vocab, gpt2merges)\n",
        "bpe_encoding = bpe.encode(\"The movie was not good at all, the climax was good though.\")\n",
        "print(bpe_encoding.ids)\n",
        "print(bpe_encoding.tokens)\n",
        "\n",
        "\n",
        "bpe = ByteLevelBPETokenizer(gpt2vocab, gpt2merges)\n",
        "bpe_encoding = bpe.encode(\"That's an example, don't ignore it!. Or else, you might miss key information\")\n",
        "print(bpe_encoding.ids)\n",
        "print(bpe_encoding.tokens)"
      ],
      "metadata": {
        "id": "ulgmqF_ZHTYD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "533f98fb-d697-43b1-ee80-dc9720a7827c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[464, 3807, 373, 407, 922, 379, 477, 11, 262, 30032, 373, 922, 996, 13]\n",
            "['The', 'Ġmovie', 'Ġwas', 'Ġnot', 'Ġgood', 'Ġat', 'Ġall', ',', 'Ġthe', 'Ġclimax', 'Ġwas', 'Ġgood', 'Ġthough', '.']\n",
            "[2504, 338, 281, 1672, 11, 836, 470, 8856, 340, 43179, 1471, 2073, 11, 345, 1244, 2051, 1994, 1321]\n",
            "['That', \"'s\", 'Ġan', 'Ġexample', ',', 'Ġdon', \"'t\", 'Ġignore', 'Ġit', '!.', 'ĠOr', 'Ġelse', ',', 'Ġyou', 'Ġmight', 'Ġmiss', 'Ġkey', 'Ġinformation']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer to Question 6:**\n",
        "Sometype of 'Ġ' is being added to the tokens excpet the tokens starting with uppercae character and tokens starting with punciations. I think this is caused because the 'Ġ' identifies the beginnig of the new word but it is not required for the first word.\n",
        ".The \"'s\" in second sentence is encoded together. While 'it' and '!' are stored separately. \n",
        "In the first sentence 'was' appears twice, so the bpe_encoding.ids are same both times. \n",
        "I think the cause of my observations is pre-defined functions being used."
      ],
      "metadata": {
        "id": "5dLOpj9amC9Q"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-K9BsX3KcC6y"
      },
      "source": [
        "## **References**\n",
        "* https://spacy.io/usage/spacy-101\n",
        "* https://spacy.io/models/en\n",
        "* https://www.nltk.org/howto/wordnet.html\n",
        "* https://www.nltk.org/_modules/nltk/stem/wordnet.html\n",
        "*https://colab.research.google.com/drive/10gwzRY55JqzgeEQOX6nwFs6bQ84-mB9f?usp=sharing#scrollTo=DP1xuStV0fDl\n",
        "*https://towardsdatascience.com/a-comprehensive-guide-to-subword-tokenisers-4bbd3bad9a7c\n",
        "*https://www.analyticsvidhya.com/blog/2019/09/demystifying-bert-groundbreaking-nlp-framework/"
      ]
    }
  ]
}