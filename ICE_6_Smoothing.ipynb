{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "pytorch",
      "language": "python",
      "name": "pytorch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "e9i_gEWgnll4",
        "PLOdDUqbVvAK",
        "BKH9dekAVvAR",
        "Xst-E5LRtCH-",
        "iIn71ZYNhmNa",
        "GgU_zdAKXjqW",
        "UcSHXm_Mw-Yh",
        "ni8rNv3M0WTZ"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ICE - 6: Smoothing Methods\n",
        "**Instructions**:\n",
        "1. Wherever you are asked to code, insert a text block below your code block and explain what you have coded as per your own understanding.\n",
        "2. If the code is provided by us, execute it, and add below a text block and provide your explanation in your own words.\n",
        "3. Submit both the .ipynb and pdf files to canvas.\n",
        "4. **The similarity score should be less than 15%.**"
      ],
      "metadata": {
        "id": "I9bhnjBN3Qzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "hwL67hHJmLvM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c154f65e-6e6b-43b9-c1a5-47b82704eea2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task-1 (10 points)\n",
        "\n",
        "Explain how Smoothing deals with 0 probability N-grams and Out of Vocabulary words. Support your explanation with examples."
      ],
      "metadata": {
        "id": "e9i_gEWgnll4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer**\n",
        "***\n",
        "We usually deal with 0 probablity uisng Laplace Smoothing(Add 1 smoothing).Pretend    we    saw    each    word    one    more    time    than    we    did. Just    add    one    to    all    the    counts!. We can also use \"Stupid backoff\"(No discount just, relative frequencies). "
      ],
      "metadata": {
        "id": "jOElTQ_dpbQU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLOdDUqbVvAK"
      },
      "source": [
        "#Tutorial 1:\n",
        "# Smoothing techniques used in Natural Language Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcZBiCxzVvAQ"
      },
      "source": [
        "In this notebook, I will introduce several smoothing techniques commonly used in NLP or machine learning algorithms. They are:\n",
        "- Laplacian (add-one) Smoothing\n",
        "- Lidstone (add-k) Smoothing\n",
        "- Absolute Discounting\n",
        "- Katz Backoff\n",
        "- Kneser-Ney Smoothing\n",
        "- Interpolation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKH9dekAVvAR"
      },
      "source": [
        "# Implementation of Smoothing methods\n",
        "-> The first step is to build an Ngram model.\n",
        "<br>-> Then we spply the smoothing methods.\n",
        "<br>-> Finally applying the evaluation metrics to judge the results obtained."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuM6Z1EQVvAS"
      },
      "source": [
        "Now, let's define some notations used in the following programs. In this notebook, **token** is the number of words in a document or a sentence, **vocab** is the number of different type of words in a document or sentence. For example, in the following sentence, there are 10 tokens and 8 vocabs (because \"I\" and \"like\" occur two times).  \n",
        "\"I like natural language processing and I like machine learning.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JJafQ56VvAS"
      },
      "source": [
        "## Bigram Language Model\n",
        "Any Ngram model can be implemented. Here we chose Bigram to not make it too complex or too simple."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCBAg5QWVvAT"
      },
      "source": [
        "from collections import defaultdict\n",
        "from collections import Counter\n",
        "from numpy.random import choice \n",
        "from tqdm import tqdm\n",
        "\n",
        "class Bigram():\n",
        "    def __init__(self):\n",
        "        self.bigram_counts = defaultdict(Counter)\n",
        "        self.unigram_counts = Counter()\n",
        "        self.context = defaultdict(Counter)\n",
        "        self.start_count = 0\n",
        "        self.token_count = 0\n",
        "        self.vocab_count = 0\n",
        "    \n",
        "    def convert_sentence(self, sentence):\n",
        "        return [\"<s>\"] + [w.lower() for w in sentence] + [\"</s>\"]\n",
        "    \n",
        "    def get_counts(self, sentences):\n",
        "        # collect unigram counts\n",
        "        for sentence in sentences:\n",
        "            sentence = self.convert_sentence(sentence)\n",
        "            for word in sentence[1:]:  # from 1, because we don't need the <s> token\n",
        "                self.unigram_counts[word] += 1\n",
        "            self.start_count += 1\n",
        "            \n",
        "        # collect bigram counts\n",
        "        for sentence in sentences:\n",
        "            sentence = self.convert_sentence(sentence)\n",
        "            bigram_list = zip(sentence[:-1], sentence[1:])\n",
        "            for bigram in bigram_list:\n",
        "                self.bigram_counts[bigram[0]][bigram[1]] += 1\n",
        "                self.context[bigram[1]][bigram[0]] += 1\n",
        "        self.token_count = sum(self.unigram_counts.values())\n",
        "        self.vocab_count = len(self.unigram_counts.keys())\n",
        "        \n",
        "    def generate_sentence(self):\n",
        "        current_word = \"<s>\"\n",
        "        sentence = [current_word]\n",
        "        while current_word != \"</s>\":\n",
        "            prev_word = current_word\n",
        "            prev_word_counts = self.bigram_counts[prev_word]\n",
        "            # obtain bigram probability distribution given the previous word\n",
        "            bigram_probs = []\n",
        "            total_counts = float(sum(prev_word_counts.values()))\n",
        "            for word in prev_word_counts:\n",
        "                bigram_probs.append(prev_word_counts[word] / total_counts)\n",
        "            # sample the next word\n",
        "            current_word = choice(list(prev_word_counts.keys()), p=bigram_probs)\n",
        "            sentence.append(current_word)\n",
        "            \n",
        "        sentence = \" \".join(sentence[1:-1])\n",
        "        return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVd1d-g-VvAV"
      },
      "source": [
        "Now that we have our Bigram model ready, let us generate our corpus using NLTK library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcnMz0qZVvAW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ce69463-b845-4c91-eff9-1ac53cf25f53"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "nltk.download('brown')\n",
        "\n",
        "bigram = Bigram()\n",
        "bigram.get_counts(brown.sents())\n",
        "for i in range(1,6):\n",
        "    print(\"Sentence %d\" % i)\n",
        "    print(bigram.generate_sentence())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1\n",
            "john's unprecedented speed , smith , lincoln , said , of sorts of new york district was to like a personal event of anglo-americans with a way with normal people who is at the design facets of a hole , where jed , canada .\n",
            "Sentence 2\n",
            "early gospelers in item 12 months or reduced , is handy at the mexicans hiding from time to several of the department than another intelligent criticism of those who had ruled .\n",
            "Sentence 3\n",
            "eugene was doing this is far out of `` we lived for me -- into the animals suggests that something to the batting coach .\n",
            "Sentence 4\n",
            "not too .\n",
            "Sentence 5\n",
            "everybody but inexperienced youth the transfer is , and buckhead .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2 (5 points)\n",
        "Build/Generate your own Corpus.You will use it in the tasks below.\n",
        "<br><br>\n",
        "**Note:** Your corpus can be as simple as a 100 word paragraph or it can be as complex as a big data set. In the above coding block, corpus was set as \"brown corpus\" which is available in the NLTK library."
      ],
      "metadata": {
        "id": "Xst-E5LRtCH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here\n",
        "#corpus=\"\"\n",
        "corpus=\"Jane Austen’s Pride and Prejudice is an 18th-century novel of manners set in rural England and portraying the relationships between the four daughters of the Bennet family and their neighbors. While accurately and vividly depicting the manners and social norms of that time, the novel also provides sharp observations on the themes of love, marriage, class, money, education, and social prestige. In this paper, the four main themes of Pride and Prejudice are analyzed. Marriage is the main topic around which the plot revolves. The author illustrates the conflict between marrying for money, which was the typical idea at the time, and marrying for love. In either case, the economic and social differences were obstacles which made it hard for young women from poor families to break out of their social circle. Each person’s position in society was determined by their class, and the relations between families also centered around differences in wealth and status. The gender differences also played an important role, as women were considered inferior to men and were practically unable to choose partners. Austen both criticizes and examines the social life of 18th-century England, advocating for marrying for love as one of the essential female rights.\"\n",
        "\n",
        "bigram = Bigram()\n",
        "bigram.get_counts(corpus)\n",
        "for i in range(1,6):\n",
        "    print(\"Sentence %d\" % i)\n",
        "    print(bigram.generate_sentence())"
      ],
      "metadata": {
        "id": "4JHu_B7AwFue",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1be0f183-e9ae-474c-d8c8-e4b5e9f3b990"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1\n",
            "w\n",
            "Sentence 2\n",
            "f\n",
            "Sentence 3\n",
            "n\n",
            "Sentence 4\n",
            "m\n",
            "Sentence 5\n",
            "e\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lzDhB6KcJqwY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHvwaX-TVvAX"
      },
      "source": [
        "The output for our sample sentence looks reasonable, Now, let's use perplexity to evaluate different smoothing techniques at the level of the corpus. For this, we'll divide Reuters corpus up randomly into a training set and a test set based on an 80/20 split. The perplexity can be calculated as follow:\n",
        "\n",
        "$PP(W) = \\sqrt[m]{\\frac{1}{P(W)}}$\n",
        "\n",
        "$\\log{PP(W)} = -\\frac{1}{m} \\log{P(W)}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLm-OjAYVvAY"
      },
      "source": [
        "import math\n",
        "from random import shuffle\n",
        "\n",
        "def split_train_test():\n",
        "    sents = list(brown.sents())\n",
        "    shuffle(sents)\n",
        "    cutoff = int(0.8*len(sents))\n",
        "    training_set = sents[:cutoff]\n",
        "    test_set = [[word.lower() for word in sent] for sent in sents[cutoff:]]\n",
        "    return training_set, test_set\n",
        "\n",
        "def calculate_perplexity(sentences, bigram, smoothing_function, parameter):\n",
        "    total_log_prob = 0\n",
        "    test_token_count = 0\n",
        "    for sentence in tqdm(sentences):\n",
        "        test_token_count += len(sentence) + 1 # have to consider the end token\n",
        "        total_log_prob += smoothing_function(sentence, bigram, parameter)\n",
        "    return math.exp(-total_log_prob / test_token_count)\n",
        "\n",
        "training_set, test_set = split_train_test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REpNJQxvVvAZ"
      },
      "source": [
        "Until Now, we can evaluate the quality of different smoothing methods via calculating perplexity of test set. Now let's start to learn these smoothing techniques. For better understanding, here we use a sample example to explain these smoothing methods. Supposing we have 7 vocabs and their counts are as follows: **(Note this is a simplified example which is more like a unigram model)**\n",
        "\n",
        "vocabs | counts | unsmoothed probability\n",
        ":-: | :-: | :-: \n",
        "impropriety | 8 | 0.4 | \n",
        "offense | 5 | 0.25 | \n",
        "damage | 4 | 0.2 | \n",
        "deficiencies | 2 | 0.1 | \n",
        "outbreak | 1 | 0.05 | \n",
        "infirmity | 0 | 0 | \n",
        "cephalopods | 0 | 0 | \n",
        "**total** | **20** | **1.0** | \n",
        "\n",
        "A bigram model without any smoothing can be formulated as follow: \n",
        "$$ P(w_{i}|w_{i-1}) = \\frac{C(w_{i-1}, w_{i})}{C(w_{i-1})} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWXbd7ALVvAZ"
      },
      "source": [
        "## Laplacian (add-one) Smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4QZAlehVvAa"
      },
      "source": [
        "Laplacian (add-one) smoothing: \n",
        "\n",
        "$$ P_{add-1}(w_{i}|w_{i-1}) = \\frac{C(w_{i-1}, w_{i}) + 1}{C(w_{i-1}) + |V|}$$\n",
        "\n",
        "**Core idea**: Pretend that we have seen each vocab at least once."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmYql3ThVvAa"
      },
      "source": [
        "vocabs | counts | unsmoothed probability | laplacian (add-one) smoothing\n",
        ":-: | :-: | :-: | :-: \n",
        "impropriety | 8 | 0.4 | (8+1)/(20+7)= 0.333\n",
        "offense | 5 | 0.25 | (5+1)/(20+7)= 0.222\n",
        "damage | 4 | 0.2 | (4+1)/(20+7)= 0.186\n",
        "deficiencies | 2 | 0.1 | (2+1)/(20+7)= 0.111\n",
        "outbreak | 1 | 0.05 | (1+1)/(20+7)= 0.074\n",
        "infirmity | 0 | 0 | (0+1)/(20+7)= 0.037\n",
        "cephalopods | 0 | 0 | (0+1)/(20+7)= 0.037\n",
        "**total** | **20** | **1.0** | **1.0**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sjaQQ1oVvAa",
        "outputId": "ee510a6f-f537-49f2-b13a-e37a67a759ac"
      },
      "source": [
        "def laplacian_smoothing(sentence, bigram, parameter):\n",
        "    sentence = bigram.convert_sentence(sentence)\n",
        "    bigram_list = zip(sentence[:-1], sentence[1:])\n",
        "    prob = 0\n",
        "    for prev_word, word in bigram_list:\n",
        "        sm_bigram_counts = bigram.bigram_counts[prev_word][word] + 1\n",
        "        if prev_word == \"<s>\": sm_unigram_counts = bigram.start_count\n",
        "        else: sm_unigram_counts = bigram.unigram_counts[prev_word] + len(bigram.unigram_counts)\n",
        "        prob += math.log(sm_bigram_counts / sm_unigram_counts)\n",
        "    return prob\n",
        "\n",
        "bigram_laplacian_smoothing = Bigram()\n",
        "bigram_laplacian_smoothing.get_counts(training_set)\n",
        "plex_laplacian_smoothing = calculate_perplexity(test_set, bigram_laplacian_smoothing, laplacian_smoothing, None)\n",
        "print(plex_laplacian_smoothing)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11468/11468 [00:00<00:00, 23217.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3509.139613536556\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdiAxWt_VvAb"
      },
      "source": [
        "## Lidstone (add-k) Smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9sGT8GHVvAc"
      },
      "source": [
        "Lidstone (add-k) smoothing: \n",
        "\n",
        "$$ P_{add-k}(w_{i}|w_{i-1}) = \\frac{C(w_{i-1}, w_{i}) + k}{C(w_{i-1}) + k|V|}$$\n",
        "\n",
        "**Core idea**: Sometimes adding one is too much, instead, we add k (usually k < 1)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpeR0W3WVvAd"
      },
      "source": [
        "vocabs | counts | unsmoothed probability | lidstone (add-k) smoothing (k=0.05)\n",
        ":-: | :-: | :-: | :-: \n",
        "impropriety | 8 | 0.4 | (8+0.5)/(20+7*0.5)= 0.363\n",
        "offense | 5 | 0.25 | (5+0.5)/(20+7*0.5)= 0.234\n",
        "damage | 4 | 0.2 | (4+0.5)/(20+7*0.5)= 0.191\n",
        "deficiencies | 2 | 0.1 | (2+0.5)/(20+7*0.5)= 0.106\n",
        "outbreak | 1 | 0.05 | (1+0.5)/(20+7*0.5)= 0.064\n",
        "infirmity | 0 | 0 | (0+0.5)/(20+7*0.5)= 0.021\n",
        "cephalopods | 0 | 0 | (0+0.5)/(20+7*0.5)= 0.021\n",
        "**total** | **20** | **1.0** | **1.0**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8EJAp6IVvAd",
        "outputId": "242a45d2-136b-4ef7-ed69-b4fcb714dfab"
      },
      "source": [
        "def lidstone_smoothing(sentence, bigram, k):\n",
        "    sentence = bigram.convert_sentence(sentence)\n",
        "    bigram_list = zip(sentence[:-1], sentence[1:])\n",
        "    prob = 0\n",
        "    for prev_word, word in bigram_list:\n",
        "        sm_bigram_counts = bigram.bigram_counts[prev_word][word] + k\n",
        "        if prev_word == \"<s>\": sm_unigram_counts = bigram.start_count\n",
        "        else: sm_unigram_counts = bigram.unigram_counts[prev_word] + k*len(bigram.unigram_counts)\n",
        "        prob += math.log(sm_bigram_counts / sm_unigram_counts)\n",
        "    return prob\n",
        "\n",
        "bigram_lidstone_smoothing = Bigram()\n",
        "bigram_lidstone_smoothing.get_counts(training_set)\n",
        "plex_lidstone_smoothing = calculate_perplexity(test_set, bigram_lidstone_smoothing, lidstone_smoothing, 0.05)\n",
        "print(plex_lidstone_smoothing)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11468/11468 [00:00<00:00, 25258.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1183.23568198887\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_Vgg4PTVvAe"
      },
      "source": [
        "## Absolute Discounting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7SHhLYTVvAe"
      },
      "source": [
        "Absolute discounting:\n",
        "\n",
        "$$ P_{absolute-discounting}(w_{i}|w_{i-1})=\\left\\{\n",
        "\\begin{aligned}\n",
        "\\frac{C(w_{i-1}, w_{i}) - D}{C(w_{i-1})}, if \\quad C(w_{i-1}, w_{i}) > 0 \\\\\n",
        "\\alpha(w_{i-1}) / \\sum\\nolimits_{w_{j}:C(w_{i-1}, w_{j})=0}, otherwise\n",
        "\\end{aligned}\n",
        "\\right.\n",
        "$$\n",
        "\n",
        "**Core idea**: 'Borrows' a fixed probability mass from observed n-gram counts and redistributes it to unseen n-grams.\n",
        "\n",
        "$\\alpha(w_{i-1})$ is the amount of probability mass that has been discounted for context $w_{i-1}$, in this example, its valuse is (0.1*5)/20.\n",
        "\n",
        "$\\sum\\nolimits_{w_{j}:C(w_{i-1}, w_{j})=0}$ is the count of $C(w_{i-1}, w_{j})=0$, here it is 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJ7zmx8SVvAe"
      },
      "source": [
        "vocabs | counts | unsmoothed probability | absolute discounting (d=0.1) | effective counts\n",
        ":-: | :-: | :-: | :-: | :-: \n",
        "impropriety | 8 | 0.4 | (8-0.1)/20=0.395 | 7.9\n",
        "offense | 5 | 0.25 | (5-0.1)/20=0.245 | 4.9\n",
        "damage | 4 | 0.2 | (4-0.1)/20=0.195 | 3.9\n",
        "deficiencies | 2 | 0.1 | (2-0.1)/20=0.095 | 1.9\n",
        "outbreak | 1 | 0.05 | (1-0.1)/20=0.045 | 0.9\n",
        "infirmity | 0 | 0 | (0+0.5)/20/2=0.0125 | 0.25\n",
        "cephalopods | 0 | 0 | (0+0.5)/20/2=0.0125 | 0.25\n",
        "**total** | **20** | **1.0** | **1.0** | **20**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTNmceDBVvAe",
        "outputId": "8bd8f10f-c934-4022-ff20-560f8d47acad"
      },
      "source": [
        "def absolute_discounting(sentence, bigram, d):\n",
        "    sentence = bigram.convert_sentence(sentence)\n",
        "    bigram_list = zip(sentence[:-1], sentence[1:])\n",
        "    prob = 0\n",
        "\n",
        "    for prev_word, word in bigram_list:\n",
        "        sm_bigram_counts = bigram.bigram_counts[prev_word][word]\n",
        "        if prev_word == \"<s>\": sm_unigram_counts = bigram.start_count\n",
        "        else: sm_unigram_counts = bigram.unigram_counts[prev_word]\n",
        "        if sm_unigram_counts == 0: \n",
        "            prob += math.log((1 / float(bigram.vocab_count)) * 0.01)\n",
        "            continue\n",
        "        if sm_bigram_counts != 0: \n",
        "            sm_bigram_counts = sm_bigram_counts - d\n",
        "        else: \n",
        "            alpha_prev_word = len(bigram.bigram_counts[prev_word].keys())\n",
        "            # count how many vocabs do not appear after pre_word\n",
        "            prev_word_discounting = bigram.vocab_count - alpha_prev_word\n",
        "            sm_bigram_counts = alpha_prev_word * d / prev_word_discounting\n",
        "        prob += math.log(sm_bigram_counts / sm_unigram_counts)\n",
        "    return prob\n",
        "\n",
        "bigram_absolute_discounting = Bigram()\n",
        "bigram_absolute_discounting.get_counts(training_set)\n",
        "plex_absolute_discounting = calculate_perplexity(test_set, bigram_absolute_discounting, absolute_discounting, 0.1)\n",
        "print(plex_absolute_discounting)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11468/11468 [00:00<00:00, 24432.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1001.3295839824883\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo9RyzXoVvAf"
      },
      "source": [
        "## Katz Backoff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnAKK43FVvAg"
      },
      "source": [
        "Katz Backoff:\n",
        "\n",
        "$$ P_{backoff}(w_{i}|w_{i-1})=\\left\\{\n",
        "\\begin{aligned}\n",
        "\\frac{C(w_{i-1}, w_{i}) - D}{C(w_{i-1})}, if \\quad C(w_{i-1}, w_{i}) > 0 \\\\\n",
        "\\alpha(w_{i-1}) \\times \\frac{P(w_{j})}{\\sum\\nolimits_{w_{j}:C(w_{i-1}, w_{j})=0}{P(w_{j})}}, otherwise\n",
        "\\end{aligned}\n",
        "\\right.\n",
        "$$\n",
        "\n",
        "**Core idea**: Absolute discounting redistributes the probability mass **equally** for all unseen n-grams while Backoff redistributes the mass based on a lower order model (e.g. unigram).\n",
        "\n",
        "$\\alpha(w_{i-1})$ is also the amount of probability mass that has been discounted for context $w_{i-1}$, in this example, its valuse is (0.1*5)/20.\n",
        "\n",
        "$P(w_{i})$ is the unigram probability for $w_{i}$. Suppose here $P(infirmity) = 0.002$, $P(cephalopods) = 0.008$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPIQCWc7VvAg"
      },
      "source": [
        "vocabs | counts | unsmoothed probability | backoff | effective counts\n",
        ":-: | :-: | :-: | :-: | :-: \n",
        "impropriety | 8 | 0.4 | (8-0.1)/20=0.395 | 7.9\n",
        "offense | 5 | 0.25 | (5-0.1)/20=0.245 | 4.9\n",
        "damage | 4 | 0.2 | (4-0.1)/20=0.195 | 3.9\n",
        "deficiencies | 2 | 0.1 | (2-0.1)/20=0.095 | 1.9\n",
        "outbreak | 1 | 0.05 | (1-0.1)/20=0.045 | 0.9\n",
        "infirmity | 0 | 0 | (0+0.5)/20*0.002/(0.002+0.008)=0.0005 | 0.1\n",
        "cephalopods | 0 | 0 | (0+0.5)/20*0.008/(0.002+0.008)=0.02 | 0.4\n",
        "**total** | **20** | **1.0** | **1.0** | **20**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sRWCd9rVvAh",
        "outputId": "26568e26-a0c9-42ba-8128-9483c85b9215"
      },
      "source": [
        "def backoff(sentence, bigram, d):\n",
        "    sentence = bigram.convert_sentence(sentence)\n",
        "    bigram_list = zip(sentence[:-1], sentence[1:])\n",
        "    prob = 0\n",
        "\n",
        "    for prev_word, word in bigram_list:\n",
        "        sm_bigram_counts = bigram.bigram_counts[prev_word][word]\n",
        "        if prev_word == \"<s>\": sm_unigram_counts = bigram.start_count\n",
        "        else: sm_unigram_counts = bigram.unigram_counts[prev_word]\n",
        "        if sm_unigram_counts == 0: \n",
        "            prob += math.log((1 / float(bigram.vocab_count)) * 0.01)\n",
        "            continue\n",
        "        if sm_bigram_counts != 0: \n",
        "            sm_bigram_counts = sm_bigram_counts - d\n",
        "        else: \n",
        "            alpha_prev_word = len(bigram.bigram_counts[prev_word].keys())\n",
        "            # sum unigram counts of word j which do not appear after pre_word\n",
        "            unseen_unigram_sum = 0\n",
        "            for vocab in bigram.unigram_counts.keys():\n",
        "                if vocab not in bigram.bigram_counts[prev_word].keys():\n",
        "                    unseen_unigram_sum += bigram.unigram_counts[vocab]\n",
        "            unseen_unigram = bigram.unigram_counts[word] / unseen_unigram_sum\n",
        "            if unseen_unigram == 0: unseen_unigram = 1 / float(bigram.vocab_count - alpha_prev_word)\n",
        "            sm_bigram_counts = alpha_prev_word * d * unseen_unigram\n",
        "        prob += math.log(sm_bigram_counts / sm_unigram_counts)\n",
        "    return prob\n",
        "\n",
        "bigram_backoff = Bigram()\n",
        "bigram_backoff.get_counts(training_set)\n",
        "plex_backoff = calculate_perplexity(test_set, bigram_backoff, backoff, 0.1)\n",
        "print(plex_backoff)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11468/11468 [22:51<00:00,  8.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "582.0024617427518\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wry0JS37VvAi"
      },
      "source": [
        "## Kneser-Ney Smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qLbTWhPVvAi"
      },
      "source": [
        "Kneser-Ney Smoothing:\n",
        "\n",
        "$$ P_{kneser-ney-smoothing}(w_{i}|w_{i-1})=\\left\\{\n",
        "\\begin{aligned}\n",
        "\\frac{C(w_{i-1}, w_{i}) - D}{C(w_{i-1})}, if \\quad C(w_{i-1}, w_{i}) > 0 \\\\\n",
        "\\alpha(w_{i-1})P_{cont}(w_{i}), otherwise\n",
        "\\end{aligned}\n",
        "\\right.\\\\\n",
        "where \\quad\n",
        "P_{cont}(w_{i}) = \\frac{|\\{w_{i-1}:C(w_{i-1}, w_{i}) > 0\\}|}{{\\sum_{w_{i}}{|\\{w_{i-1}:C(w_{i-1}, w_{i}) > 0\\}|}}}\n",
        "$$\n",
        "\n",
        "**Core idea**: Redistribute probability mass based on how many number of different contexts word w has appeared in.\n",
        "\n",
        "$\\alpha(w_{i-1})$ is also the amount of probability mass that has been discounted for context $w_{i-1}$, in this example, its valuse is (0.1*5)/20.  \n",
        "Suppose we have the following phrases in the corpus: {A infirmity, B infirmity, C infirmity, D infirmity, A cephalopods}, then  \n",
        "$|\\{w_{i-1}:C(w_{i-1}, w_{i}) > 0\\}|$ for $w_{i}$ = infirmity is 4, $P_{cont}(w_{i}=infirmity)$ = 4/(4+1)= 0.8.  \n",
        "$|\\{w_{i-1}:C(w_{i-1}, w_{i}) > 0\\}|$ for $w_{i}$ = cephalopods is 1, $P_{cont}(w_{i}=cephalopods)$ = 1/(4+1)= 0.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pq7TxWOSVvAi"
      },
      "source": [
        "vocabs | counts | unsmoothed probability | kneser-ney smoothing | effective counts\n",
        ":-: | :-: | :-: | :-: | :-: \n",
        "impropriety | 8 | 0.4 | (8-0.1)/20=0.395 | 7.9\n",
        "offense | 5 | 0.25 | (5-0.1)/20=0.245 | 4.9\n",
        "damage | 4 | 0.2 | (4-0.1)/20=0.195 | 3.9\n",
        "deficiencies | 2 | 0.1 | (2-0.1)/20=0.095 | 1.9\n",
        "outbreak | 1 | 0.05 | (1-0.1)/20=0.045 | 0.9\n",
        "infirmity | 0 | 0 | (0+0.5)/20*4/(4+1)=0.02 | 0.4\n",
        "cephalopods | 0 | 0 | (0+0.5)/20*1/(4+1)=0.005 | 0.1\n",
        "**total** | **20** | **1.0** | **1.0** | **20**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ji5SJpXZVvAi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "outputId": "d864ca5a-6e12-42dd-84cb-7fd6e0d75963"
      },
      "source": [
        "def kneser_ney_smoothing(sentence, bigram, d):\n",
        "    sentence = bigram.convert_sentence(sentence)\n",
        "    bigram_list = zip(sentence[:-1], sentence[1:])\n",
        "    prob = 0\n",
        "\n",
        "    for prev_word, word in bigram_list:\n",
        "        sm_bigram_counts = bigram.bigram_counts[prev_word][word]\n",
        "        if prev_word == \"<s>\": sm_unigram_counts = bigram.start_count\n",
        "        else: sm_unigram_counts = bigram.unigram_counts[prev_word]\n",
        "        if sm_unigram_counts == 0: \n",
        "            prob += math.log((1 / float(bigram.vocab_count)) * 0.01)\n",
        "            continue\n",
        "        if sm_bigram_counts != 0: \n",
        "            sm_bigram_counts = sm_bigram_counts - d\n",
        "        else: \n",
        "            # statistic how many tokens not occureed after pre_word\n",
        "            alpha_prev_word = len(bigram.bigram_counts[prev_word].keys())\n",
        "            \n",
        "            context_sum = 0\n",
        "            for vocab in bigram.unigram_counts.keys():\n",
        "                if vocab not in bigram.bigram_counts[prev_word].keys():\n",
        "                    context_sum += len(bigram.context[vocab].keys())\n",
        "            p_cont = len(bigram.context[word].keys()) / context_sum\n",
        "            if p_cont == 0: p_cont = 1 / float(bigram.vocab_count - alpha_prev_word)\n",
        "            sm_bigram_counts = alpha_prev_word * d * p_cont\n",
        "        prob += math.log(sm_bigram_counts / sm_unigram_counts)\n",
        "    return prob\n",
        "\n",
        "bigram_kneser_ney_smoothing = Bigram()\n",
        "bigram_kneser_ney_smoothing.get_counts(training_set)\n",
        "plex_kneser_ney_smoothing = calculate_perplexity(test_set, bigram_kneser_ney_smoothing, kneser_ney_smoothing, 0.1)\n",
        "print(plex_kneser_ney_smoothing)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 128/11468 [00:27<40:28,  4.67it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-82bc27b8bd67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mbigram_kneser_ney_smoothing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBigram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mbigram_kneser_ney_smoothing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mplex_kneser_ney_smoothing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_perplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbigram_kneser_ney_smoothing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkneser_ney_smoothing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplex_kneser_ney_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-efdb7309fd75>\u001b[0m in \u001b[0;36mcalculate_perplexity\u001b[0;34m(sentences, bigram, smoothing_function, parameter)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mtest_token_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m# have to consider the end token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mtotal_log_prob\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msmoothing_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbigram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtotal_log_prob\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtest_token_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-82bc27b8bd67>\u001b[0m in \u001b[0;36mkneser_ney_smoothing\u001b[0;34m(sentence, bigram, d)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0msm_bigram_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbigram\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbigram_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprev_word\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprev_word\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"<s>\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msm_unigram_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbigram\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msm_unigram_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbigram\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munigram_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprev_word\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msm_unigram_counts\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mprob\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigram\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rifVT0J1VvAi"
      },
      "source": [
        "## Interpolation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxovDxSNVvAj"
      },
      "source": [
        "Interpolation:\n",
        "\n",
        "$$ \n",
        "\\begin{aligned}\n",
        "P_{interpolation}(w_{i}|w_{i-1}, w_{i-2})&=\\lambda_{3}P_{3}(w_{i}|w_{i-1}, w_{i-2}) \\\\\n",
        "&+\\lambda_{2}P_{2}(w_{i}|w_{i-1})\\\\\n",
        "&+\\lambda_{1}P_{1}(w_{i})\\\\\n",
        "where \\quad\n",
        "\\sum_{i}{\\lambda_{i}} = 1\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "**Core idea**: Combine different order n-gram models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zm2j-acPVvAj"
      },
      "source": [
        "def interpolation(sentence, bigram, lambdas):\n",
        "    bigram_lambda = lambdas[0]\n",
        "    unigram_lambda = lambdas[1]\n",
        "    zerogram_lambda = 1 - lambdas[0] - lambdas[1]\n",
        "    \n",
        "    sentence = bigram.convert_sentence(sentence)\n",
        "    bigram_list = zip(sentence[:-1], sentence[1:])\n",
        "    prob = 0\n",
        "    \n",
        "    for prev_word, word in bigram_list:\n",
        "        # bigram probability\n",
        "        sm_bigram_counts = bigram.bigram_counts[prev_word][word]\n",
        "        if sm_bigram_counts == 0: interp_bigram_counts = 0\n",
        "        else:\n",
        "            if prev_word == \"<s>\": u_counts = bigram.start_count\n",
        "            else: u_counts = bigram.unigram_counts[prev_word]\n",
        "            interp_bigram_counts = sm_bigram_counts / float(u_counts) * bigram_lambda\n",
        "\n",
        "        # unigram probability\n",
        "        interp_unigram_counts = (bigram.unigram_counts[word] / bigram.token_count) * unigram_lambda\n",
        "\n",
        "        # \"zerogram\" probability: this is to account for out-of-vocabulary words, this is just 1 / |V|\n",
        "        vocab_size = len(bigram.unigram_counts)\n",
        "        interp_zerogram_counts = (1 / float(vocab_size)) * zerogram_lambda\n",
        "    \n",
        "        prob += math.log(interp_bigram_counts + interp_unigram_counts + interp_zerogram_counts)\n",
        "    return prob\n",
        "\n",
        "bigram_interpolation = Bigram()\n",
        "bigram_interpolation.get_counts(training_set)\n",
        "plex_interpolation = calculate_perplexity(test_set, bigram_interpolation, interpolation, (0.8, 0.19))\n",
        "print(plex_interpolation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08DqeLzeVvAj"
      },
      "source": [
        "Now we have finished our work, the following table shows the perplexity of different smoothing methods. We can learn that different smoothing techniques may greatly affect the quality of language models. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fv-L-E4XVvAj"
      },
      "source": [
        "smoothing techniques | perpleity on Brown test corpus\n",
        ":-: | :-: \n",
        "Laplacian (add-one) Smoothing | 3508\n",
        "Lidstone (add-k) Smoothing | 1188\n",
        "Absolute Discounting | 1013\n",
        "Katz Backoff | 588\n",
        "Kneser-Ney Smoothing | 569\n",
        "Interpolation | 436"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLU8Zyf2VvAk"
      },
      "source": [
        "The above implementations may not be optimal according to efficiency and memory, but it shows how different smoothing techniques work in a language model intuitively, so it may be a good tutorial for some beginners of NLP. If there are some mistakes in the code, welcome to point it out and I will correct it as soon as possible."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3 (10 points)\n",
        "\n",
        "Write a detailed anlysis for all the smoothing methods present above. Support your analysis specific evidences."
      ],
      "metadata": {
        "id": "iIn71ZYNhmNa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "***\n",
        "The ranking from Worst to Best is:\n",
        "Worst- Laplacian(add-one) smoothing\n",
        "       ,Lindstone(add-k) Smoothing\n",
        "       ,Absolute Discounting\n",
        "       ,Katz Backoff\n",
        "       ,Kneser-Ney Smoothing\n",
        ",Best-  Interpolation"
      ],
      "metadata": {
        "id": "Xp7u1Yscrgmp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4 (5 points)\n",
        "Explain the key differences between Interpolation and Backoff method. "
      ],
      "metadata": {
        "id": "WypxPGXDbdi7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "***\n",
        "In Interpolation we mix(use together) unigram,bigram and trigram. But in Backoff method we use trigram if we have good evidence, otherwise bigram, otherwise unigram. Interpolation usually workds better "
      ],
      "metadata": {
        "id": "_1z3KG25hdu7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NOTE\n",
        "\n",
        "For Tasks 6, 7 and 8, you will have to choose Smoothing methods of your own choice and implement them according to the corresponding question. Make sure you choose different Smoothing methods across all three questions."
      ],
      "metadata": {
        "id": "SjDGJpw1s94K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 5 (10 points)\n",
        "\n",
        "For this task, you need to change the percentage of train data and test data. In the split_train_test() method present above, make the changes necessary. **Make sure you use this updated method for implementing all the Tasks below.**"
      ],
      "metadata": {
        "id": "btttAp5MiURz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here\n",
        "import math\n",
        "from random import shuffle\n",
        "\n",
        "def split_train_test():\n",
        "    sents = list(brown.sents())\n",
        "    shuffle(sents)\n",
        "    cutoff = int(0.75*len(sents))\n",
        "    training_set = sents[:cutoff]\n",
        "    test_set = [[word.lower() for word in sent] for sent in sents[cutoff:]]\n",
        "    return training_set, test_set\n",
        "\n",
        "def calculate_perplexity(sentences, bigram, smoothing_function, parameter):\n",
        "    total_log_prob = 0\n",
        "    test_token_count = 0\n",
        "    for sentence in tqdm(sentences):\n",
        "        test_token_count += len(sentence) + 1 # have to consider the end token\n",
        "        total_log_prob += smoothing_function(sentence, bigram, parameter)\n",
        "    return math.exp(-total_log_prob / test_token_count)\n",
        "\n",
        "training_set, test_set = split_train_test()"
      ],
      "metadata": {
        "id": "OsBRQR-2kIxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data has split into 75/25. 75% training data and 25% test data."
      ],
      "metadata": {
        "id": "u-zOyaoDK_7g"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgU_zdAKXjqW"
      },
      "source": [
        "# Task 6 (20 points)\n",
        "\n",
        "Implement any **two** of the above implemented smoothing methods now for a Trigram model. You can refer to code in previous ICEs."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here\n",
        "import nltk  #I had to add this line because I was getting an error NLTK is imported\n",
        "nltk.download('reuters') # I had to add this line because I was getting an error. Reuters is downloaded\n",
        "nltk.download('punkt') #I had to add this line because I was getting an error. Punkt is downloaded\n",
        "from nltk.corpus import reuters\n",
        "from nltk import trigrams"
      ],
      "metadata": {
        "id": "FoYUnopIrm4I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5faa603-92f5-4cb4-9d57-d27d0982efe2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def laplacian_smoothing(sentence, trigrams, parameter):\n",
        "    sentence = bigram.convert_sentence(sentence)\n",
        "    bigram_list = zip(sentence[:-1], sentence[1:])\n",
        "    prob = 0\n",
        "    for prev_word, word in bigram_list:\n",
        "        sm_bigram_counts = bigram.bigram_counts[prev_word][word] + 1\n",
        "        if prev_word == \"<s>\": sm_unigram_counts = bigram.start_count\n",
        "        else: sm_unigram_counts = bigram.unigram_counts[prev_word] + len(bigram.unigram_counts)\n",
        "        prob += math.log(sm_bigram_counts / sm_unigram_counts)\n",
        "    return prob\n",
        "\n",
        "bigram_laplacian_smoothing = Bigram()\n",
        "bigram_laplacian_smoothing.get_counts(training_set)\n",
        "plex_laplacian_smoothing = calculate_perplexity(test_set, bigram_laplacian_smoothing, laplacian_smoothing, None)\n",
        "print(plex_laplacian_smoothing)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOq-90C4PfLu",
        "outputId": "aa40617c-7789-416d-90bb-62483c3d64da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11468/11468 [00:00<00:00, 13568.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2617.584337258673\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def absolute_discounting(sentence, trigrams, d):\n",
        "    sentence = bigram.convert_sentence(sentence)\n",
        "    bigram_list = zip(sentence[:-1], sentence[1:])\n",
        "    prob = 0\n",
        "\n",
        "    for prev_word, word in bigram_list:\n",
        "        sm_bigram_counts = bigram.bigram_counts[prev_word][word]\n",
        "        if prev_word == \"<s>\": sm_unigram_counts = bigram.start_count\n",
        "        else: sm_unigram_counts = bigram.unigram_counts[prev_word]\n",
        "        if sm_unigram_counts == 0: \n",
        "            prob += math.log((1 / float(bigram.vocab_count)) * 0.01)\n",
        "            continue\n",
        "        if sm_bigram_counts != 0: \n",
        "            sm_bigram_counts = sm_bigram_counts - d\n",
        "        else: \n",
        "            alpha_prev_word = len(bigram.bigram_counts[prev_word].keys())\n",
        "            # count how many vocabs do not appear after pre_word\n",
        "            prev_word_discounting = bigram.vocab_count - alpha_prev_word\n",
        "            sm_bigram_counts = alpha_prev_word * d / prev_word_discounting\n",
        "        prob += math.log(sm_bigram_counts / sm_unigram_counts)\n",
        "    return prob\n",
        "\n",
        "bigram_absolute_discounting = Bigram()\n",
        "bigram_absolute_discounting.get_counts(training_set)\n",
        "plex_absolute_discounting = calculate_perplexity(test_set, bigram_absolute_discounting, absolute_discounting, 0.1)\n",
        "print(plex_absolute_discounting)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqNRTdFNQuXZ",
        "outputId": "66f90744-2375-4a4a-80c5-416727f0ef4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11468/11468 [00:00<00:00, 23941.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "74.26577526883358\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 7 (20 points)\n",
        "\n",
        "\n",
        "Implement any smoothing technique which was implemented above for your own corpus which you have generated in Task 2.\n",
        "\n",
        "Steps:\n",
        "1. Split the corpus into test and train sets using the split_train_test() method present in the tutorial above. **Note: Observe how the brown corpus was accessed in the code, how the brown corpus's sentences were accessed etc, for deeper understanding.**\n",
        "2. Implement any one of the smoothing methods.\n",
        "3. Calculate the perplexity.\n",
        "\n",
        "**Note: For this task, do not just make function calls. Copy and paste the necessary code blocks from the tutorial which are necessary, and show the work flow of the implementation. Do not choose the same method that you have chosen in Task 6.**"
      ],
      "metadata": {
        "id": "UcSHXm_Mw-Yh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code hereimport math\n",
        "from random import shuffle\n",
        "\n",
        "def split_train_test():\n",
        "    sents = list(corpus)\n",
        "    shuffle(sents)\n",
        "    cutoff = int(0.75*len(sents))\n",
        "    training_set = sents[:cutoff]\n",
        "    test_set = [[word.lower() for word in sent] for sent in sents[cutoff:]]\n",
        "    return training_set, test_set\n",
        "\n",
        "def calculate_perplexity(sentences, bigram, smoothing_function, parameter):\n",
        "    total_log_prob = 0\n",
        "    test_token_count = 0\n",
        "    for sentence in tqdm(sentences):\n",
        "        test_token_count += len(sentence) + 1 # have to consider the end token\n",
        "        total_log_prob += smoothing_function(sentence, bigram, parameter)\n",
        "    return math.exp(-total_log_prob / test_token_count)\n",
        "\n",
        "training_set, test_set = split_train_test()"
      ],
      "metadata": {
        "id": "PnX4iEaRw9ey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def absolute_discounting(sentence, bigram, d):\n",
        "    sentence = bigram.convert_sentence(sentence)\n",
        "    bigram_list = zip(sentence[:-1], sentence[1:])\n",
        "    prob = 0\n",
        "\n",
        "    for prev_word, word in bigram_list:\n",
        "        sm_bigram_counts = bigram.bigram_counts[prev_word][word]\n",
        "        if prev_word == \"<s>\": sm_unigram_counts = bigram.start_count\n",
        "        else: sm_unigram_counts = bigram.unigram_counts[prev_word]\n",
        "        if sm_unigram_counts == 0: \n",
        "            prob += math.log((1 / float(bigram.vocab_count)) * 0.01)\n",
        "            continue\n",
        "        if sm_bigram_counts != 0: \n",
        "            sm_bigram_counts = sm_bigram_counts - d\n",
        "        else: \n",
        "            alpha_prev_word = len(bigram.bigram_counts[prev_word].keys())\n",
        "            # count how many vocabs do not appear after pre_word\n",
        "            prev_word_discounting = bigram.vocab_count - alpha_prev_word\n",
        "            sm_bigram_counts = alpha_prev_word * d / prev_word_discounting\n",
        "        prob += math.log(sm_bigram_counts / sm_unigram_counts)\n",
        "    return prob\n",
        "\n",
        "bigram_absolute_discounting = Bigram()\n",
        "bigram_absolute_discounting.get_counts(training_set)\n",
        "plex_absolute_discounting = calculate_perplexity(test_set, bigram_absolute_discounting, absolute_discounting, 0.1)\n",
        "print(plex_absolute_discounting)"
      ],
      "metadata": {
        "id": "xsOLD2BgMso0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 8 (20 points)\n",
        "\n",
        "Implement any **one** of the above implemented smoothing methods now for a Unigram model. You can refer to code in previous ICEs.\n",
        "\n",
        "**Note: Do not choose the same method that you have chosen in Task 6 or 7**"
      ],
      "metadata": {
        "id": "ni8rNv3M0WTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here\n",
        "import collections\n",
        "from nltk.tokenize import word_tokenize\n",
        "# First text corpus is tokenized\n",
        "tokens = word_tokenize(corpus)\n",
        "\n",
        "#unigram language model \n",
        "def unigram(tokens):    \n",
        "    model = collections.defaultdict(lambda: 0.01)\n",
        "    for f in tokens:\n",
        "        try:\n",
        "            model[f] += 1\n",
        "        except KeyError:\n",
        "            model [f] = 1\n",
        "            continue\n",
        "    N = float(sum(model.values()))\n",
        "    for word in model:\n",
        "        model[word] = model[word]/N\n",
        "    return model"
      ],
      "metadata": {
        "id": "pb0EpTvDqTJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def interpolation(sentence, unigram, lambdas):\n",
        "    bigram_lambda = lambdas[0]\n",
        "    unigram_lambda = lambdas[1]\n",
        "    zerogram_lambda = 1 - lambdas[0] - lambdas[1]\n",
        "    \n",
        "    sentence = bigram.convert_sentence(sentence)\n",
        "    bigram_list = zip(sentence[:-1], sentence[1:])\n",
        "    prob = 0\n",
        "    \n",
        "    for prev_word, word in bigram_list:\n",
        "        # bigram probability\n",
        "        sm_bigram_counts = bigram.bigram_counts[prev_word][word]\n",
        "        if sm_bigram_counts == 0: interp_bigram_counts = 0\n",
        "        else:\n",
        "            if prev_word == \"<s>\": u_counts = bigram.start_count\n",
        "            else: u_counts = bigram.unigram_counts[prev_word]\n",
        "            interp_bigram_counts = sm_bigram_counts / float(u_counts) * bigram_lambda\n",
        "\n",
        "        # unigram probability\n",
        "        interp_unigram_counts = (bigram.unigram_counts[word] / bigram.token_count) * unigram_lambda\n",
        "\n",
        "        # \"zerogram\" probability: this is to account for out-of-vocabulary words, this is just 1 / |V|\n",
        "        vocab_size = len(bigram.unigram_counts)\n",
        "        interp_zerogram_counts = (1 / float(vocab_size)) * zerogram_lambda\n",
        "    \n",
        "        prob += math.log(interp_bigram_counts + interp_unigram_counts + interp_zerogram_counts)\n",
        "    return prob\n",
        "\n",
        "bigram_interpolation = Bigram()\n",
        "bigram_interpolation.get_counts(training_set)\n",
        "plex_interpolation = calculate_perplexity(test_set, bigram_interpolation, interpolation, (0.8, 0.19))\n",
        "print(plex_interpolation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQNgnP_aNMxB",
        "outputId": "f46fc27c-b7dd-4270-e21c-8b4bbb9cbd3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11468/11468 [00:00<00:00, 17580.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "83.48760509356943\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}